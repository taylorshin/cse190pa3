# mdp implementation needs to go hereimport mathimport Queuefrom copy import deepcopyfrom read_config import read_configconfig = read_config()# Read inputsmove_list = config['move_list']map_size = config['map_size']start = config['start']goal = config['goal']walls = config['walls']pits = config['pits']max_iterations = config['max_iterations']threshold_difference = config['threshold_difference']reward_for_each_step = config['reward_for_each_step']reward_for_hitting_wall = config['reward_for_hitting_wall']reward_for_reaching_goal = config['reward_for_reaching_goal']reward_for_falling_in_pit = config['reward_for_falling_in_pit']discount_factor = config['discount_factor']prob_move_forward = config['prob_move_forward']prob_move_backward = config['prob_move_backward']prob_move_left = config['prob_move_left']prob_move_right = config['prob_move_right']values = {}""" Markov decision process """def mdp():    policy_list = []    direction_list = ['E', 'W', 'S', 'N']    # Start with all values at 0    for row in xrange(map_size[0]):        for col in xrange(map_size[1]):            state = (row, col)            if is_goal(state):                values[state] = reward_for_reaching_goal            elif is_pit(state):                values[state] = reward_for_falling_in_pit            elif is_wall(state):                values[state] = reward_for_hitting_wall            else:                values[state] = 0        # Value iteration    for x in xrange(1, max_iterations):        policies = []        # Values of the previous iteration        prev_values = deepcopy(values)        # Value iterations for all states        for row in xrange(map_size[0]):            policy_row = []            for col in xrange(map_size[1]):                curr_state = (row, col)                if is_goal(curr_state):                    policy_row.append('GOAL')                elif is_pit(curr_state):                    policy_row.append('PIT')                elif is_wall(curr_state):                    policy_row.append('WALL')                else:                    actions = []                    # Move list has order of east, west, south, north                    count = 0                    for move in move_list:                        # Forward in respect to the move                        next_forward = (curr_state[0] + move[0], curr_state[1] + move[1])                        forward_val = prob_move_forward * calculate_utility(curr_state, next_forward)                        # Backward in respect to the move                        next_backward = (curr_state[0] - move[0], curr_state[1] - move[1])                        backward_val = prob_move_backward * calculate_utility(curr_state, next_backward)                        # Left in respect to the move                        next_left = (curr_state[0] - move[1], curr_state[1] + move[0])                        left_val = prob_move_left * calculate_utility(curr_state, next_left)                        # Right in respect to the move                        next_right = (curr_state[0] + move[1], curr_state[1] - move[0])                        right_val = prob_move_right * calculate_utility(curr_state, next_right)                        # Sum up to pick the max                        actions.append(((forward_val + backward_val + left_val + right_val), direction_list[count]))                        count += 1                    # Calculate optimal value V*                    optimal = max(actions, key=lambda x:x[0])                    values[curr_state] = optimal[0]                    policy_row.append(optimal[1])            policies.append(policy_row)        policy_list.append(policies)        # Stop value iteration if MDP algorithm converges        if convergence_reached(values, prev_values):            return policy_list    return policy_list""" Checks if MDP convergence has been reached """def convergence_reached(values, prev_values):    sum = 0    for key in values.keys():        sum += math.fabs(values[key] - prev_values[key])    if sum < threshold_difference:        return True    return False""" Calculate the utility of a state depending on its type """def calculate_utility(curr_state, next_state):    utility = 0    if is_goal(next_state):        utility = reward_for_each_step + reward_for_reaching_goal + discount_factor * values[next_state]    elif is_pit(next_state):        utility = reward_for_each_step + reward_for_falling_in_pit + discount_factor * values[next_state]    elif is_wall(next_state):        utility = reward_for_hitting_wall + discount_factor * values[curr_state]    else:        utility = reward_for_each_step + discount_factor * values[next_state]    return utility""" Check if goal """def is_goal(state):    if state[0] == goal[0] and state[1] == goal[1]:        return True    return False""" Check if location is a wall """def is_wall(loc):    # Out of bounds    if loc[0] < 0 or loc[0] >= map_size[0] or loc[1] < 0 or loc[1] >= map_size[1]:        return True    for wall in walls:        if (loc[0] == wall[0]) and (loc[1] == wall[1]):            return True    return False""" Check if location is a pit """def is_pit(loc):    for pit in pits:        if (loc[0] == pit[0]) and (loc[1] == pit[1]):            return True    return False